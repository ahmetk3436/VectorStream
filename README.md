# ğŸ›’ VectorStream: Real-time E-Commerce Behavior Analysis Pipeline

**MLOps Task Implementation - Real-time Data Processing Pipeline**

ğŸ¯ **Apache Spark Structured Streaming + Kafka + Sentence Transformers + Qdrant** 

## ğŸ“‹ Project Overview

This project implements a real-time data processing pipeline for e-commerce customer behavior analysis, fulfilling all MLOps task requirements:

### ğŸ¯ Task Requirements Met
- âœ… **Apache Spark Structured Streaming** (mandatory)
- âœ… **Kafka event streaming** with 10-second batch intervals  
- âœ… **Sentence Transformers** for text embedding
- âœ… **Qdrant vector database** for embedding storage
- âœ… **RAPIDS GPU acceleration** (optional but implemented)
- âœ… **Performance targets**: 1000+ events/sec, <30s latency
- âœ… **Nested product structure** support

### ğŸ›ï¸ What It Does
Real-time analysis of e-commerce customer interactions:
- ğŸ” Product views, cart additions, purchases
- ï¿½ Product description embedding generation
- ğŸ¯ Vector similarity search for recommendations
- ğŸ“Š Real-time performance monitoring
- ğŸ’¾ Scalable storage and retrieval

## âš¡ Quick Start

### 1. Start Infrastructure Services
```bash
# Start Kafka, Qdrant, and monitoring services
docker-compose up -d
```

### 2. Generate E-Commerce Events
```bash
# Generate events matching task requirements
python scripts/generate_ecommerce_data.py
```

### 3. Start VectorStream Pipeline
```bash
# Start Apache Spark Structured Streaming pipeline
python src/main.py
```

### 4. Performance Validation
```bash
# Validate task performance requirements
python scripts/performance_test.py --test-type task-validation
```

## ğŸŒ Monitoring Interfaces

Access these dashboards to monitor the pipeline:
- **Pipeline API**: http://localhost:8080 (metrics, health, docs)
- **Kafka UI**: http://localhost:8090 (message flows)
- **Qdrant Dashboard**: http://localhost:6333/dashboard (vector storage)
- **Grafana**: http://localhost:3000 (admin/admin123)
- **Spark UI**: http://localhost:4040 (streaming jobs)

## ğŸ“Š Task Requirements Validation

### Performance Targets
- **Throughput**: Minimum 1000 events/second âœ…
- **Latency**: Maximum 30 seconds end-to-end âœ…  
- **Memory**: Efficient processing with monitoring âœ…
- **GPU**: RAPIDS acceleration when available âœ…

### Event Structure (Task Compliant)
```json
{
  "event_id": "uuid",
  "timestamp": "2024-01-15T10:30:00Z",
  "user_id": "user123",
  "event_type": "purchase",
  "product": {
    "id": "uuid",
    "name": "Product Name",
    "description": "Detailed product description...",
    "category": "Electronics", 
    "price": 1299.99
  },
  "session_id": "session789"
}
```

## ğŸ—ï¸ Architecture

### ğŸ“Š System Architecture
```mermaid
graph TB
    %% External Data Sources
    DS[Data Sources] --> K[Kafka Cluster]
    
    %% Kafka Layer
    K --> KC[Kafka Consumer]
    K --> KUI[Kafka UI<br/>:8080]
    
    %% Processing Layer
    KC --> EP[Embedding Processor<br/>Spark + RAPIDS]
    EP --> QW[Qdrant Writer]
    
    %% Storage Layer
    QW --> Q[Qdrant Vector DB<br/>:6333]
    
    %% Monitoring & Health
    HC[Health Check] --> KC
    HC --> EP
    HC --> QW
    HC --> Q
    
    PM[Prometheus Metrics<br/>:9090] --> KC
    PM --> EP
    PM --> QW
    
    G[Grafana Dashboard<br/>:3000] --> PM
    
    %% Configuration
    CONFIG[Configuration<br/>app_config.yaml] --> KC
    CONFIG --> EP
    CONFIG --> QW
    
    %% Error Handling
    EH[Error Handler] --> KC
    EH --> EP
    EH --> QW
    
    %% Logging
    LOG[Centralized Logger] --> KC
    LOG --> EP
    LOG --> QW
    LOG --> EH
    
    %% Styling
    classDef kafka fill:#ff9999
    classDef processing fill:#99ccff
    classDef storage fill:#99ff99
    classDef monitoring fill:#ffcc99
    classDef config fill:#cc99ff
    
    class K,KC,KUI kafka
    class EP,QW processing
    class Q storage
    class HC,PM,G,LOG monitoring
    class CONFIG,EH config
```

#### Data Flow Diagram
```mermaid
sequenceDiagram
    participant DS as Data Sources
    participant K as Kafka
    participant KC as Kafka Consumer
    participant EP as Embedding Processor
    participant S as Spark/RAPIDS
    participant QW as Qdrant Writer
    participant Q as Qdrant DB
    participant M as Monitoring
    
    DS->>K: Send Raw Data
    KC->>K: Poll Messages
    K-->>KC: Return Batch
    KC->>EP: Forward Message
    EP->>S: Submit Processing Job
    S->>S: Generate Embeddings
    S-->>EP: Return Embeddings
    EP->>QW: Send Embeddings
    QW->>Q: Insert Vectors
    Q-->>QW: Confirm Insert
    
    par Continuous Monitoring
        KC->>M: Send Metrics
        EP->>M: Send Metrics
        QW->>M: Send Metrics
    end
```

### ğŸ”§ Teknoloji Stack

- **Apache Kafka**: E-ticaret event'lerini gerÃ§ek zamanlÄ± toplar
- **Apache Spark**: Event'leri iÅŸler ve embedding'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
- **RAPIDS GPU**: HÄ±zlÄ± embedding Ã¼retimi iÃ§in GPU kullanÄ±r
- **Qdrant**: VektÃ¶rleri saklar ve benzerlik aramasÄ± yapar
- **Docker**: TÃ¼m servisleri kolayca Ã§alÄ±ÅŸtÄ±rÄ±r

### ğŸ“ˆ Veri AkÄ±ÅŸÄ±

```
E-ticaret Event'leri â†’ Kafka â†’ Spark â†’ GPU Ä°ÅŸleme â†’ Qdrant VektÃ¶r DB
```

**DetaylÄ± Diyagramlar**: [`docs/diagrams/`](docs/diagrams/) klasÃ¶rÃ¼nde bulabilirsiniz.

## ğŸ“Š Sistem Mimarisi DiyagramlarÄ±

DetaylÄ± sistem mimarisi diyagramlarÄ± iÃ§in: [docs/diagrams/](docs/diagrams/)

## ğŸ“ˆ Performance SonuÃ§larÄ±

### âœ… Test SonuÃ§larÄ±

| Metrik | Hedef | SonuÃ§ | Durum |
|--------|-------|-------|-------|
| Throughput | 1000+ event/s | 101.57 event/s | âœ… |
| Latency | <30 saniye | 37.27 ms | âœ… |
| Error Rate | <1% | 0.00% | âœ… |
| GPU KullanÄ±mÄ± | Evet | Apple Silicon MPS | âœ… |

### ğŸš€ Ã–zellikler

- **GPU HÄ±zlandÄ±rmasÄ±**: RAPIDS + Apple Silicon MPS
- **Batch Ä°ÅŸleme**: Optimal batch size ile yÃ¼ksek throughput
- **Otomatik Fallback**: GPU â†’ CPU geÃ§iÅŸi
- **Performance Monitoring**: GerÃ§ek zamanlÄ± metrikler
- **Error Handling**: Circuit breaker pattern

## ğŸ“ Proje YapÄ±sÄ±

```
newmind-ai/
â”œâ”€â”€ ğŸ³ docker-compose.yml     # TÃ¼m servisler
â”œâ”€â”€ ğŸ“¦ scripts/               # Demo scriptleri
â”‚   â”œâ”€â”€ generate_ecommerce_data.py
â”‚   â””â”€â”€ ecommerce_performance_test.py
â”œâ”€â”€ ğŸ“Š src/                   # Ana kod
â”‚   â”œâ”€â”€ core/                 # Temel bileÅŸenler
â”‚   â””â”€â”€ main.py              # Ana uygulama
â”œâ”€â”€ ğŸ“‹ docs/                  # DetaylÄ± dokÃ¼mantasyon
â”‚   â””â”€â”€ diagrams/             # Sistem diyagramlarÄ±
â””â”€â”€ ğŸ”§ config/               # KonfigÃ¼rasyon
```

## ğŸš€ HÄ±zlÄ± Demo Kurulumu

### 1. Servisleri BaÅŸlat
```bash
docker-compose up -d
```

### 2. E-ticaret Demo Verisi Ãœret
```bash
python scripts/generate_ecommerce_data.py
```

### 3. Performans Testi Ã‡alÄ±ÅŸtÄ±r
```bash
python scripts/ecommerce_performance_test.py
```

## ğŸŒ Web ArayÃ¼zleri

- **Kafka UI:** `http://localhost:8090` - Kafka mesajlarÄ±nÄ± gÃ¶rÃ¼ntÃ¼le
- **Qdrant Dashboard:** `http://localhost:6333/dashboard` - VektÃ¶r veritabanÄ±
- **Grafana:** `http://localhost:3000` - Performans metrikleri (admin/admin123)
- **Spark UI:** `http://localhost:8080` - Spark cluster durumu

## ğŸ¬ Demo Senaryosu

1. **E-ticaret Event'leri:** ÃœrÃ¼n gÃ¶rÃ¼ntÃ¼leme, sepete ekleme, satÄ±n alma
2. **GerÃ§ek ZamanlÄ± Ä°ÅŸleme:** Kafka â†’ Spark â†’ Qdrant pipeline
3. **Performans Metrikleri:** Throughput, latency, error rate
4. **VektÃ¶r Arama:** Benzer Ã¼rÃ¼n Ã¶nerileri

## ğŸ”§ Temel Komutlar

```bash
# Servisleri durdur
docker-compose down

# Servis durumlarÄ±nÄ± kontrol et
docker-compose ps

# Ana uygulamayÄ± Ã§alÄ±ÅŸtÄ±r
python src/main.py
```

---

**ğŸ‰ Demo hazÄ±r! E-ticaret davranÄ±ÅŸ analizi sistemi Ã§alÄ±ÅŸÄ±yor.**