#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task Uyumlu Embedding Processor
VectorStream MLOps Task iÃ§in Sentence Transformers tabanlÄ± embedding iÅŸlemcisi
"""

import asyncio
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from loguru import logger
import numpy as np
import torch

class EmbeddingProcessor:
    """
    Task gereksinimi: Sentence Transformers embedding iÅŸlemcisi
    ÃœrÃ¼n aÃ§Ä±klamalarÄ±nÄ± embedding'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    """
    
    def __init__(self, config: Dict[str, Any]):
        """Task uyumlu embedding processor baÅŸlatma"""
        self.config = config
        # Task gereksinimi: Sentence Transformers model
        self.model_name = config.get('model_name', 'all-MiniLM-L6-v2')  
        self.model: Optional[SentenceTransformer] = None
        self.vector_size = config.get('vector_size', 384)
        self.batch_size = config.get('batch_size', 32)
        self.device = self._get_best_device()
        
        logger.info(f"ğŸ¯ Task uyumlu embedding processor baÅŸlatÄ±lÄ±yor:")
        logger.info(f"   ğŸ“ Model: {self.model_name} (Sentence Transformers)")
        logger.info(f"   ğŸ“ Vector size: {self.vector_size}")
        logger.info(f"   ğŸ”¥ Device: {self.device}")
        logger.info(f"   ğŸ“¦ Batch size: {self.batch_size}")
    
    def _get_best_device(self):
        """Task performansÄ± iÃ§in en iyi device seÃ§: GPU -> MPS -> CPU"""
        if torch.cuda.is_available():
            device = 'cuda'
            logger.info("âœ… CUDA GPU bulundu - Task performansÄ± optimize")
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = 'mps'  
            logger.info("âœ… Apple MPS bulundu - Task performansÄ± optimize")
        else:
            device = 'cpu'
            logger.info("âš ï¸ Sadece CPU kullanÄ±lacak - Task performansÄ± sÄ±nÄ±rlÄ±")
        return device
    
    async def initialize(self):
        """Task gereksinimi: Sentence Transformers model yÃ¼kle"""
        try:
            logger.info(f"ğŸ”„ Task uyumlu Sentence Transformers model yÃ¼kleniyor: {self.model_name}")
            
            # Task gereksinimi: Sentence Transformers model
            self.model = SentenceTransformer(self.model_name)
            
            # GPU/MPS kullanÄ±mÄ± optimize et
            if self.device in ['cuda', 'mps']:
                self.model = self.model.to(self.device)
                logger.info(f"âœ… Model {self.device} device'a taÅŸÄ±ndÄ±")
            
            # Model bilgilerini logla
            embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"âœ… Task uyumlu Sentence Transformers model hazÄ±r:")
            logger.info(f"   ğŸ“ Embedding dimension: {embedding_dim}")
            logger.info(f"   ğŸ¯ Task gereksinimi karÅŸÄ±landÄ±: Sentence Transformers âœ“")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Task uyumlu embedding model yÃ¼kleme hatasÄ±: {e}")
            return False
    
    async def create_embedding(self, text: str) -> List[float]:
        """
        Task gereksinimi: ÃœrÃ¼n aÃ§Ä±klamalarÄ±nÄ± embedding'e dÃ¶nÃ¼ÅŸtÃ¼r
        
        Args:
            text: ÃœrÃ¼n aÃ§Ä±klamasÄ± metni
            
        Returns:
            List[float]: Sentence Transformers embedding vector
        """
        if not self.model:
            await self.initialize()
        
        try:
            # Task gereksinimi: Sentence Transformers ile embedding
            embedding = self.model.encode(
                text,
                convert_to_tensor=False,
                normalize_embeddings=True,  # Cosine similarity iÃ§in normalize
                batch_size=1
            )
            
            # Numpy array'i liste'ye Ã§evir
            if isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()
            
            # Task gereksinimi: Vector size kontrolÃ¼
            if len(embedding) != self.vector_size:
                logger.warning(f"âš ï¸ Vector size uyumsuzluÄŸu: {len(embedding)} != {self.vector_size}")
            
            return embedding
            
        except Exception as e:
            logger.error(f"âŒ Task uyumlu embedding oluÅŸturma hatasÄ±: {e}")
            # Fallback: Rastgele vector (sadece test iÃ§in)
            return np.random.rand(self.vector_size).tolist()
    
    async def create_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Task gereksinimi: Batch halinde embedding oluÅŸtur (performans iÃ§in)
        
        Args:
            texts: ÃœrÃ¼n aÃ§Ä±klamasÄ± metinleri listesi
            
        Returns:
            List[List[float]]: Sentence Transformers embedding vektÃ¶rleri
        """
        if not self.model:
            await self.initialize()
        
        try:
            # Task performans gereksinimi: Batch processing
            embeddings = self.model.encode(
                texts,
                convert_to_tensor=False,
                normalize_embeddings=True,
                batch_size=self.batch_size,
                show_progress_bar=len(texts) > 100
            )
            
            # Numpy array'i liste'ye Ã§evir
            if isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()
            
            logger.info(f"âœ… Task uyumlu batch embedding tamamlandÄ±: {len(texts)} metin")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ Task uyumlu batch embedding hatasÄ±: {e}")
            # Fallback: Rastgele vektÃ¶rler (sadece test iÃ§in)
            return [np.random.rand(self.vector_size).tolist() for _ in texts]
        device_priority = ['cuda', 'mps', 'cpu']
        
        for device in device_priority:
            try:
                logger.info(f"Embedding model yÃ¼kleniyor: {self.model_name}")
                
                # Device kontrolÃ¼
                if device == 'cuda' and not torch.cuda.is_available():
                    continue
                elif device == 'mps' and not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    continue
                
                logger.info(f"Denenen device: {device}")
                
                # Meta tensor hatasÄ± iÃ§in Ã¶zel Ã§Ã¶zÃ¼m
                model_kwargs = {
                    'trust_remote_code': True,
                    'device': None  # Device'Ä± None yapÄ±yoruz, sonra manuel olarak taÅŸÄ±yacaÄŸÄ±z
                }
                
                # Model'i Ã¶nce CPU'da yÃ¼kle
                self.model = SentenceTransformer(
                    self.model_name,
                    **model_kwargs
                )
                
                # Sonra hedef device'a taÅŸÄ±
                if device != 'cpu':
                    try:
                        # to_empty() kullanarak meta tensor hatasÄ±nÄ± Ã¶nle
                        self.model = self.model.to(device)
                    except Exception as device_error:
                        logger.warning(f"Device {device}'a taÅŸÄ±ma baÅŸarÄ±sÄ±z: {device_error}")
                        # CPU'da kalsÄ±n
                        device = 'cpu'
                
                logger.info(f"âœ… Embedding model yÃ¼klendi: {self.model_name} on {device}")
                return  # BaÅŸarÄ±lÄ± yÃ¼kleme, fonksiyondan Ã§Ä±k
                
            except Exception as e:
                logger.warning(f"Device {device} ile model yÃ¼kleme baÅŸarÄ±sÄ±z: {e}")
                if device == 'cpu':  # CPU son seÃ§enek, hata fÄ±rlat
                    logger.error(f"TÃ¼m device'larda model yÃ¼kleme baÅŸarÄ±sÄ±z")
                    raise e
                continue  # Sonraki device'Ä± dene
        
        # Buraya ulaÅŸÄ±lmamalÄ±, ama gÃ¼venlik iÃ§in
        raise Exception("HiÃ§bir device'da model yÃ¼klenemedi")
    
    async def create_embedding(self, text: str) -> Optional[List[float]]:
        """Tek metin iÃ§in embedding oluÅŸtur"""
        if not self.model:
            await self.initialize()
            
        if not text or not text.strip():
            logger.warning("BoÅŸ metin iÃ§in embedding oluÅŸturulamaz")
            return None
            
        try:
            embedding = self.model.encode(text.strip())
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Embedding oluÅŸturma hatasÄ±: {e}")
            return None
    
    async def create_embeddings(self, texts: List[str]) -> List[Optional[List[float]]]:
        """Ã‡oklu metin iÃ§in embedding oluÅŸtur"""
        if not self.model:
            await self.initialize()
            
        if not texts:
            return []
            
        try:
            # BoÅŸ metinleri filtrele
            valid_texts = [text.strip() for text in texts if text and text.strip()]
            
            if not valid_texts:
                return [None] * len(texts)
                
            embeddings = self.model.encode(valid_texts)
            
            # SonuÃ§larÄ± orijinal sÄ±rayla eÅŸleÅŸtir
            results = []
            valid_idx = 0
            
            for text in texts:
                if text and text.strip():
                    results.append(embeddings[valid_idx].tolist())
                    valid_idx += 1
                else:
                    results.append(None)
                    
            return results
            
        except Exception as e:
            logger.error(f"Batch embedding oluÅŸturma hatasÄ±: {e}")
            return [None] * len(texts)
    
    async def process_message(self, message: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """MesajÄ± iÅŸle ve embedding oluÅŸtur"""
        try:
            content = message.get('content', '')
            
            if not content:
                logger.warning(f"Mesajda content bulunamadÄ±: {message.get('id', 'unknown')}")
                return None
                
            embedding = await self.create_embedding(content)
            
            if embedding is None:
                return None
                
            return {
                'vector': embedding,
                'metadata': {
                    'id': message.get('id', ''),
                    'content': content,
                    'timestamp': message.get('timestamp', ''),
                    'source': message.get('metadata', {}).get('source', 'unknown')
                }
            }
            
        except Exception as e:
            logger.error(f"Mesaj iÅŸleme hatasÄ±: {e}")
            return None
    
    async def process_messages(self, messages: List[Dict[str, Any]]) -> List[Optional[Dict[str, Any]]]:
        """Ã‡oklu mesaj iÅŸle"""
        try:
            if not messages:
                return []
                
            # TÃ¼m metinleri topla
            texts = [msg.get('content', '') for msg in messages]
            
            # Batch embedding oluÅŸtur
            embeddings = await self.create_embeddings(texts)
            
            # SonuÃ§larÄ± hazÄ±rla
            results = []
            for i, (message, embedding) in enumerate(zip(messages, embeddings)):
                if embedding is not None:
                    result = {
                        'vector': embedding,
                        'metadata': {
                            'id': message.get('id', f'batch_{i}'),
                            'content': message.get('content', ''),
                            'timestamp': message.get('timestamp', ''),
                            'source': message.get('metadata', {}).get('source', 'unknown')
                        }
                    }
                    results.append(result)
                else:
                    results.append(None)
                    
            return results
            
        except Exception as e:
            logger.error(f"Batch mesaj iÅŸleme hatasÄ±: {e}")
            return [None] * len(messages)
    
    def extract_text_from_task_event(self, event: Dict[str, Any]) -> str:
        """Task yapÄ±sÄ±na uygun event'ten metin Ã§Ä±kar
        
        Task event yapÄ±sÄ±:
        {
            "event_id": "uuid",
            "timestamp": "2024-01-15T10:30:00Z",
            "user_id": "user123",
            "event_type": "purchase",
            "product": {
                "id": "uuid",
                "name": "ÃœrÃ¼n AdÄ±",
                "description": "DetaylÄ± Ã¼rÃ¼n aÃ§Ä±klamasÄ±...",
                "category": "Elektronik",
                "price": 1299.99
            },
            "session_id": "session789"
        }
        """
        text_parts = []
        
        # Product bilgilerini SADECE nested yapÄ±dan al (Task gereksinimi)
        product = event.get('product', {})
        if product.get('description'):
            text_parts.append(product.get('description'))
        if product.get('name'):
            text_parts.append(product.get('name'))
        if product.get('category'):
            text_parts.append(product.get('category'))
        
        # Search sorgusu varsa
        if event.get('search_query'):
            text_parts.append(event.get('search_query'))
        
        # Event type'Ä± da context olarak ekle
        if event.get('event_type'):
            text_parts.append(event.get('event_type'))
        
        text = ' '.join(text_parts) if text_parts else f"event_{event.get('event_id', 'unknown')}"
        return text

    async def process_task_event(self, event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Task gereksinimlerine uygun event iÅŸle"""
        try:
            if not self.model:
                await self.initialize()
            
            # Task event'inden metin Ã§Ä±kar
            text = self.extract_text_from_task_event(event)
            
            if not text or text.strip() == '':
                logger.warning(f"Event'ten metin Ã§Ä±karÄ±lamadÄ±: {event.get('event_id')}")
                return None
            
            # Embedding oluÅŸtur
            embedding = await self.create_embedding(text)
            
            if embedding is None:
                logger.error(f"Embedding oluÅŸturulamadÄ±: {event.get('event_id')}")
                return None
            
            # Task yapÄ±sÄ±na uygun metadata
            product = event.get('product', {})
            return {
                'vector': embedding,
                'metadata': {
                    'event_id': event.get('event_id'),
                    'timestamp': event.get('timestamp'),
                    'event_type': event.get('event_type'),
                    'user_id': event.get('user_id'),
                    'session_id': event.get('session_id'),
                    'text': text,
                    # Product bilgileri
                    'product_id': product.get('id'),
                    'product_name': product.get('name'),
                    'product_description': product.get('description'),
                    'product_category': product.get('category'),
                    'product_price': product.get('price'),
                    # Search bilgileri
                    'search_query': event.get('search_query'),
                    'results_count': event.get('results_count'),
                    # Purchase bilgileri
                    'quantity': event.get('quantity'),
                    'total_amount': event.get('total_amount'),
                    'payment_method': event.get('payment_method'),
                    'processed_at': event.get('processed_at')
                }
            }
            
        except Exception as e:
            logger.error(f"Task event iÅŸleme hatasÄ±: {e}")
            return None

# Task uyumluluÄŸu iÃ§in alias
SentenceTransformersEmbeddingProcessor = EmbeddingProcessor