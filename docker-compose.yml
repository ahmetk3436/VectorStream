services:
  kafka:
    image: apache/kafka-native:3.8.0         
    container_name: kafka
    ports:
      - "9092:9092"      
    volumes:
      - kafka-data:/var/lib/kafka/data
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@localhost:9091"
      KAFKA_LISTENERS: |
        CONTROLLER://0.0.0.0:9091,
        HOST://0.0.0.0:9092,
        DOCKER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: |
        HOST://localhost:9092,
        DOCKER://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,HOST:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    restart: unless-stopped

  kafka-ui:
    image: kafbat/kafka-ui:latest             # Opsiyonel görsel arayüz
    container_name: kafka-ui
    ports:
      - "8090:8080"
    environment:
      DYNAMIC_CONFIG_ENABLED: "true"
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9093
    depends_on:
      - kafka
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    restart: unless-stopped
  spark-master:
    image: apache/spark:latest         
    container_name: spark-master
    hostname: spark-master
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.master.Master",
              "--host", "spark-master",
              "--port", "7077"]
    env_file:                           
      - ./config/spark/spark-env.sh
    volumes:
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      - ./config/spark/spark-env.sh:/opt/spark/conf/spark-env.sh:ro
      - spark-events:/opt/spark/spark-events
      - ./src:/opt/spark/src
    ports:
      - "7077:7077"     
      - "8080:8080" 

  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.worker.Worker",
              "spark://spark-master:7077"]
    env_file:
      - ./config/spark/spark-env.sh
    volumes:
      - ./src:/opt/spark/src
      - spark-events:/opt/spark/spark-events
      - ./config/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    ports:
      - "8081:8081"  

  spark-history:
    image: apache/spark:latest
    container_name: spark-history
    hostname: spark-history
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class",
              "org.apache.spark.deploy.history.HistoryServer"]
    env_file:
      - ./config/spark/spark-env.sh
    volumes:
      - spark-events:/opt/spark/spark-events
    ports:
      - "18080:18080"  

networks:
  newmind:
    driver: bridge

volumes:
  spark-events:
  kafka-data:
  qdrant_storage: